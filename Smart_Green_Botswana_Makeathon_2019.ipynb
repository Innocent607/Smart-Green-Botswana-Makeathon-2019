{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Smart Green Botswana Makeathon 2019",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Innocent607/Smart-Green-Botswana-Makeathon-2019/blob/main/Smart_Green_Botswana_Makeathon_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOnNRXXbMEz6"
      },
      "source": [
        "# Smart Green Botswana Makeathon 2019\r\n",
        "\r\n",
        "Project: AI Based Waste Classification and Monitoring System\r\n",
        "---\r\n",
        "\r\n",
        "Author: Onalethata I. L. Maswabi\r\n",
        "\r\n",
        "Collaborators / Team Members\r\n",
        "1.   Tlamelo Makati\r\n",
        "2.   Kesego Mokgosi\r\n",
        "3.   Kesego Tumisang\r\n",
        "4.   Elias Bayona\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6oXVVF6JGao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f67cab2-e4cf-4126-97b8-1aa01e5515bd"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "import pickle\n",
        "from os import path\n",
        "import glob\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGDvhuNHJbyl"
      },
      "source": [
        "#https://drive.google.com/open?id=18atPbzldHifLI9U0RRbUZ0uGIyFGUB7U\n",
        "CATEGORIES = [\"plastics\", \"can\", \"glass\", \"paper\"]\n",
        "class Training():\n",
        "\n",
        "        def __init__(self):\n",
        "\n",
        "                self.DATADIR = os.path.abspath(\"/content/drive/My Drive/data/train/\") # path.abspath(\"data\")\n",
        "                #print(self.DATADIR)\n",
        "                # All the categories you want your neural network to detect\n",
        "                # use names of the subfolders\n",
        "                # The size of the images that your neural network will use\n",
        "                self.IMG_SIZE = 50\n",
        "\n",
        "                self.GRAY_IMG = cv2.IMREAD_GRAYSCALE #cv2.IMREAD_GRAYSCALE\n",
        "\n",
        "                # Checking or all images in the data folder\n",
        "                for category in CATEGORIES :\n",
        "                        path_categories = os.path.join(self.DATADIR, category)\n",
        "                        #print (path_categories)\n",
        "                        for img in os.listdir(path_categories):\n",
        "                                img_array = cv2.imread(os.path.join(path_categories, img), self.GRAY_IMG)\n",
        "\n",
        "        def training_data (self):\n",
        "                TRAINING_DATA = []\n",
        "                for category in CATEGORIES:\n",
        "                        path_categories = os.path.join(self.DATADIR, category)\n",
        "                        class_index = CATEGORIES.index(category)\n",
        "                        for img in os.listdir(path_categories):\n",
        "                                try:\n",
        "                                        img_array = cv2.imread(os.path.join(path_categories, img), self.GRAY_IMG)\n",
        "                                        new_img_array = cv2.resize(img_array, (self.IMG_SIZE, self.IMG_SIZE))\n",
        "                                        #plt.imshow(new_img_array)\n",
        "                                        TRAINING_DATA.append(np.array([new_img_array, class_index]))\n",
        "\n",
        "                                except Exception as e:\n",
        "                                        pass\n",
        "                #print (TRAINING_DATA)\n",
        "                return TRAINING_DATA\n",
        "\n",
        "        def testing_data (self):\n",
        "                TESTING_DATA = []\n",
        "                for category in CATEGORIES:\n",
        "                        path_categories = os.path.join(os.path.abspath(\"/content/drive/My Drive/data/test/\"), category)\n",
        "                        class_index = CATEGORIES.index(category)\n",
        "\n",
        "                        for img in os.listdir(path_categories):\n",
        "                                try:\n",
        "                                        img_array = cv2.imread(os.path.join(path_categories, img), self.GRAY_IMG)\n",
        "                                        new_img_array = cv2.resize(img_array, (self.IMG_SIZE, self.IMG_SIZE))\n",
        "                                        #plt.imshow(new_img_array)\n",
        "                                        TESTING_DATA.append([new_img_array, class_index])\n",
        "\n",
        "                                except Exception as e:\n",
        "                                        pass\n",
        "                #print (TRAINING_DATA)\n",
        "                return TESTING_DATA\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwZfBHFZcIfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed40af7f-86b0-4688-fd87-7f576005827f"
      },
      "source": [
        "t = Training()\n",
        "t.training_data()\n",
        "t.testing_data()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[array([[255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8), 0],\n",
              " [array([[  3,   3,   4, ..., 148, 181, 153],\n",
              "         [ 14,  14,  16, ..., 153, 175, 156],\n",
              "         [  6,   9,  11, ..., 158, 173, 158],\n",
              "         ...,\n",
              "         [ 99,  81, 111, ..., 105, 116, 155],\n",
              "         [ 66, 116, 128, ..., 115, 140, 111],\n",
              "         [100, 125, 149, ..., 140, 110, 101]], dtype=uint8), 0],\n",
              " [array([[255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8), 0],\n",
              " [array([[ 78, 137, 124, ..., 187, 132, 133],\n",
              "         [141, 146, 109, ..., 196, 133, 131],\n",
              "         [143, 138, 102, ..., 206, 198, 147],\n",
              "         ...,\n",
              "         [163, 169, 170, ...,  90,  98, 112],\n",
              "         [162, 166, 167, ..., 130, 110, 108],\n",
              "         [161, 167, 189, ..., 129, 102, 127]], dtype=uint8), 0],\n",
              " [array([[ 6,  4,  2, ..., 49, 42, 24],\n",
              "         [ 9,  4,  3, ..., 38, 45, 29],\n",
              "         [ 1,  6,  5, ..., 45, 76, 43],\n",
              "         ...,\n",
              "         [10, 25, 10, ..., 21, 19, 13],\n",
              "         [ 9, 14, 29, ..., 16, 15, 13],\n",
              "         [ 8, 10, 14, ..., 18, 12, 13]], dtype=uint8), 1],\n",
              " [array([[ 98, 112, 116, ...,  56,  69,  88],\n",
              "         [ 50,  19, 112, ..., 107,  51, 162],\n",
              "         [ 79,  79, 107, ..., 138, 145, 136],\n",
              "         ...,\n",
              "         [143, 166, 159, ..., 161, 137, 160],\n",
              "         [160, 118, 125, ..., 128, 141, 124],\n",
              "         [ 66, 114, 121, ..., 100, 134, 108]], dtype=uint8), 1],\n",
              " [array([[199, 201, 202, ...,  16,  15,  48],\n",
              "         [209, 202, 212, ...,  17,  13,  19],\n",
              "         [201, 205, 195, ...,  15,  16,  18],\n",
              "         ...,\n",
              "         [ 53,  53,  60, ...,  53,  53,  53],\n",
              "         [ 53,  53,  52, ...,  53,  53,  53],\n",
              "         [ 53,  53,  50, ...,  53,  53,  53]], dtype=uint8), 1],\n",
              " [array([[  7,   7,  68, ...,  90,  77,  93],\n",
              "         [ 31,  37,  24, ...,  94, 117,  54],\n",
              "         [ 28,  13,  58, ...,  67,  66,  97],\n",
              "         ...,\n",
              "         [ 31,  10,  43, ..., 124,  99, 146],\n",
              "         [113,  59,  86, ...,  47, 114,  93],\n",
              "         [ 69,  74, 100, ...,  35,  54,  97]], dtype=uint8), 1],\n",
              " [array([[ 75,  78, 165, ...,  97,  98, 101],\n",
              "         [ 85, 110,  60, ..., 100,  88, 111],\n",
              "         [ 99,  59,  57, ..., 123, 121, 134],\n",
              "         ...,\n",
              "         [132, 149, 124, ..., 118, 118, 116],\n",
              "         [155, 150, 123, ..., 128, 125, 102],\n",
              "         [145, 142, 150, ..., 128, 147,  92]], dtype=uint8), 1],\n",
              " [array([[255, 255, 255, ..., 254, 254, 254],\n",
              "         [255, 255, 255, ..., 254, 254, 254],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255, ..., 254, 254, 254],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8), 1],\n",
              " [array([[188, 187, 131, ..., 108,  76,  46],\n",
              "         [180, 173, 161, ...,  73,  70,  48],\n",
              "         [179, 184, 180, ..., 118, 143,  92],\n",
              "         ...,\n",
              "         [ 53,  42, 198, ...,  61,  50,  53],\n",
              "         [ 53,  48, 212, ..., 130,  61,  53],\n",
              "         [ 53,  50,  54, ...,  55,  52,  53]], dtype=uint8), 2],\n",
              " [array([[ 58,  71,  62, ..., 255, 245, 243],\n",
              "         [ 54,  69,  70, ..., 255, 255, 235],\n",
              "         [ 45,  82,  70, ..., 252, 247, 235],\n",
              "         ...,\n",
              "         [138, 148, 146, ..., 122, 102,  74],\n",
              "         [147, 146, 164, ..., 118, 112,  71],\n",
              "         [151, 155, 181, ...,  41, 119,  62]], dtype=uint8), 2],\n",
              " [array([[105, 106, 103, ..., 103,  97, 113],\n",
              "         [105, 106, 103, ..., 107, 108,  93],\n",
              "         [104, 107, 104, ..., 110, 110,  99],\n",
              "         ...,\n",
              "         [159, 151, 155, ...,  93, 130, 131],\n",
              "         [162, 159, 121, ..., 129, 119, 135],\n",
              "         [161, 137, 111, ..., 121, 121, 124]], dtype=uint8), 2],\n",
              " [array([[255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255, ..., 253, 251, 255],\n",
              "         [255, 254, 252, ..., 255, 255, 255],\n",
              "         [255, 255, 246, ..., 251, 255, 255]], dtype=uint8), 3],\n",
              " [array([[246, 246, 246, ..., 246, 246, 246],\n",
              "         [246, 246, 246, ..., 246, 246, 246],\n",
              "         [246, 246, 246, ..., 246, 246, 246],\n",
              "         ...,\n",
              "         [246, 246, 246, ..., 246, 246, 246],\n",
              "         [246, 246, 246, ..., 246, 246, 246],\n",
              "         [246, 246, 246, ..., 246, 246, 246]], dtype=uint8), 3],\n",
              " [array([[255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255],\n",
              "         [255, 255, 255, ..., 255, 255, 255]], dtype=uint8), 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOD39--dLghF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a674616-f39c-4346-d6cb-b5b7c22e2d0b"
      },
      "source": [
        "Y, y, X, x = [], [], [], [] # labels, fatures\n",
        "\n",
        "training_data = Training().training_data()\n",
        "testing_data = Training().testing_data()\n",
        "random.shuffle(training_data)\n",
        "\n",
        "def create_y_x ():\n",
        "\n",
        "        for features, labels in training_data:\n",
        "\n",
        "                X.append(features)\n",
        "                Y.append(labels)\n",
        "\n",
        "        for features, labels in testing_data:\n",
        "                x.append(features)\n",
        "                y.append(labels)\n",
        "\n",
        "        x_train = np.array(X).reshape(-1, 50, 50, 1)\n",
        "        x_test = np.array(x).reshape(-1, 50, 50, 1)\n",
        "\n",
        "\n",
        "        # create model files\n",
        "        pickle_out = open(\"X_train.pickle\", \"wb\")\n",
        "        pickle.dump(x_train, pickle_out)\n",
        "        pickle_out.close()\n",
        "\n",
        "        pickle_out = open(\"X_test.pickle\", \"wb\")\n",
        "        pickle.dump(x_test, pickle_out)\n",
        "        pickle_out.close()\n",
        "\n",
        "        pickle_out = open(\"y_train.pickle\", \"wb\")\n",
        "        pickle.dump(Y, pickle_out)\n",
        "        pickle_out.close()\n",
        "\n",
        "        pickle_out = open(\"y_test.pickle\", \"wb\")\n",
        "        pickle.dump(y, pickle_out)\n",
        "        pickle_out.close()\n",
        "\n",
        "        pickle_in = open(\"X_train.pickle\", \"rb\")\n",
        "        x_train = pickle.load(pickle_in)\n",
        "\n",
        "        pickle_in = open(\"X_test.pickle\", \"rb\")\n",
        "        x_test = pickle.load(pickle_in)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG6DdGsQKoxU"
      },
      "source": [
        "create_y_x()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyDEeFP3G7lY"
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "import pickle\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE7yovKtHpcD"
      },
      "source": [
        "X_train = pickle.load(open(\"X_train.pickle\", \"rb\"))\n",
        "y_train = pickle.load(open(\"y_train.pickle\", \"rb\"))\n",
        "\n",
        "X_test = pickle.load(open(\"X_test.pickle\", \"rb\"))\n",
        "y_test = pickle.load(open(\"y_test.pickle\", \"rb\"))\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w8jOX4QeI_Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "08c2e382-1a8a-47ca-baa0-9878dd0af834"
      },
      "source": [
        "# Building the model\n",
        "model = models.Sequential()\n",
        "# 3 convolutional layers\n",
        "model.add(layers.Conv2D(32, (3, 3), input_shape = np.array(X_train).shape[1:]))\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3)))\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3)))\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "# 4 hidden layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128))\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "\n",
        "model.add(layers.Dense(128))\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "\n",
        "model.add(layers.Dense(100))\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "\n",
        "model.add(layers.Dense(128))\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "\n",
        "# The output layer with 13 neurons, for 13 classes\n",
        "model.add(layers.Dense(4))\n",
        "model.add(layers.Activation(\"softmax\"))\n",
        "\n",
        "# Compiling the model using some basic parameters\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "\t\t\t\toptimizer=\"adam\",\n",
        "\t\t\t\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# Training the model, with 40 iterations\n",
        "# validation_split corresponds to the percentage of images used for the validation phase compared to all the images\n",
        "history = model.fit(X_train, y_train, batch_size=30, epochs=50, validation_split=0.1)\n",
        "\n",
        "# Saving the model\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file :\n",
        "\tjson_file.write(model_json)\n",
        "\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "model.save('CNN.model')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-48e168f6d4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Training the model, with 40 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# validation_split corresponds to the percentage of images used for the validation phase compared to all the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Saving the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1039\u001b[0m       (x, y, sample_weight), validation_data = (\n\u001b[1;32m   1040\u001b[0m           data_adapter.train_validation_split(\n\u001b[0;32m-> 1041\u001b[0;31m               (x, y, sample_weight), validation_split=validation_split))\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     raise ValueError(\n\u001b[1;32m   1358\u001b[0m         \u001b[0;34m\"`validation_split` is only supported for Tensors or NumPy \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \"arrays, found following types in the input: {}\".format(unsplitable))\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n2nXONvnSHg"
      },
      "source": [
        "\n",
        "# Printing a graph showing the accuracy changes during the training phase\n",
        "print(history.history.keys())\n",
        "plt.figure(1)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSiVPJVJq_Rp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENhurOTKn9re"
      },
      "source": [
        "# https://dzone.com/articles/object-detection-tutorial-in-tensorflow-real-time\n",
        "\n",
        "file_path = os.path.abspath(\"/content/drive/My Drive/data/test/can/\")\n",
        "model = tf.keras.models.load_model(\"CNN.model\")\n",
        "def prepare(file_path):\n",
        "    for img in os.listdir(file_path):\n",
        "          img_array = cv2.imread(os.path.join(file_path, img), cv2.IMREAD_GRAYSCALE)\n",
        "          new_img_array = cv2.resize(img_array, (50, 50))\n",
        "          plt.imshow(img_array)\n",
        "\n",
        "    return np.array(new_img_array).reshape(-1, 50, 50, 1)\n",
        "\n",
        "\"\"\"\n",
        "new_model = load_model('CNN.model', custom_objects=None, compile=True)\n",
        "prediction = new_model.predict([X_test])\n",
        "print(prediction)\n",
        "\"\"\"\n",
        "\n",
        "#model = tf.keras.models.load_model(\"CNN.model\")\n",
        "#image = prepare(file_path) # \"images(5).jpg\" #your image path\n",
        "val_loss, val_acc = model.evaluate(X_test,y_test)\n",
        "print(val_loss, val_acc)\n",
        "prediction = model.predict(prepare(file_path))\n",
        "prediction = list(prediction[0])\n",
        "print(CATEGORIES[prediction.index(max(prediction))])\n",
        "#test_loss, test_acc = model.evaluate(X_test,  \"glass\", verbose=2)\n",
        "\n",
        "\n",
        "#prepare(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzibZBVIGUyo"
      },
      "source": [
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):      \n",
        "    if 'detection_masks' in tensor_dict:\n",
        "        # The following processing is only for single image\n",
        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "        detection_masks_reframed = tf.cast(\n",
        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "        # Follow the convention by adding back the batch dimension\n",
        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "            detection_masks_reframed, 0)\n",
        "    image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "    # Run inference\n",
        "    output_dict = sess.run(tensor_dict,\n",
        "                           feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "    # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "    output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "    output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.uint8)\n",
        "    output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "    output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "    if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct_6up6HGYGi"
      },
      "source": [
        "\n",
        "!pip install Image\n",
        "import Image\n",
        "detection_graph = tf.Graph()\n",
        "TEST_IMAGE_PATHS = os.path.abspath(\"/content/gdrive/My Drive/data/test/plastics/\")\n",
        "!pip install opencv-python==3.4.4.19\n",
        "with detection_graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "        ops = tf.get_default_graph().get_operations()\n",
        "        all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "        tensor_dict = {}\n",
        "        for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "          ]:\n",
        "            tensor_name = key + ':0'\n",
        "            if tensor_name in all_tensor_names:\n",
        "                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
        "        for image_path in TEST_IMAGE_PATHS:\n",
        "            image = Image.open(image_path)\n",
        "            # the array based representation of the image will be used later in order to prepare the\n",
        "            # result image with boxes and labels on it.\n",
        "            image_np = load_image_into_numpy_array(image)\n",
        "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "            # Actual detection.\n",
        "            output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "            # Visualization of the results of a detection.\n",
        "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "                image_np,\n",
        "                output_dict['detection_boxes'],\n",
        "                output_dict['detection_classes'],\n",
        "                output_dict['detection_scores'],\n",
        "                category_index,\n",
        "                instance_masks=output_dict.get('detection_masks'),\n",
        "                use_normalized_coordinates=True,\n",
        "                line_thickness=8)\n",
        "            plt.figure(figsize=IMAGE_SIZE)\n",
        "            plt.imshow(image_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvd1lB5JGrtM"
      },
      "source": [
        "import cv2\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "with detection_graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "        ops = tf.get_default_graph().get_operations()\n",
        "        all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "        tensor_dict = {}\n",
        "        for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "          ]:\n",
        "            tensor_name = key + ':0'\n",
        "            if tensor_name in all_tensor_names:\n",
        "                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "        while True:\n",
        "            ret, image_np = cap.read()\n",
        "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "            # Actual detection.\n",
        "            output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "            # Visualization of the results of a detection.\n",
        "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "              image_np,\n",
        "              output_dict['detection_boxes'],\n",
        "              output_dict['detection_classes'],\n",
        "              output_dict['detection_scores'],\n",
        "              category_index,\n",
        "              instance_masks=output_dict.get('detection_masks'),\n",
        "              use_normalized_coordinates=True,\n",
        "              line_thickness=8)\n",
        "            cv2.imshow('object detection', cv2.resize(image_np, (800,600)))\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                cap.release()\n",
        "                cv2.destroyAllWindows()\n",
        "                break\n",
        "\n",
        "A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPXksH-g_Q-9"
      },
      "source": [
        "\n",
        "#testing_data = Training().testing_data()    \n",
        "#print (testing_data[]) \n",
        "!pip install mrcnn    \n",
        "from mrcnn import visualize\n",
        "import skimage.io\n",
        "import random\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "\n",
        "\n",
        "\n",
        "img_dir = os.path.abspath(\"/content/gdrive/My Drive/data/test/paper/\")\n",
        "file_names = next(os.walk(img_dir))[2]\n",
        "image = skimage.io.imread(os.path.join(img_dir, random.choice(file_names)))\n",
        "model = tf.keras.models.load_model(\"CNN.model\")\n",
        "#results = model.detect([image], verbose=1)\n",
        "#r = results[0]\n",
        "visualize.display_instances(image) #, r['rois'], r['masks'], C, \n",
        "                            class_names, CATEGORIES)\n",
        "\"\"\"\n",
        "import time\n",
        "def test():\n",
        "  for category in CATEGORIES:\n",
        "    path_categories = os.path.join(os.path.abspath(\"/content/gdrive/My Drive/data/test/\"), category)\n",
        "    image = skimage.io.imread(path_categories)\n",
        "    results = model.detect([image], verbose=1)\n",
        "\n",
        "    class_index = CATEGORIES.index(category)\n",
        "\n",
        "    for img in os.listdir(path_categories):\n",
        "      img_array = cv2.imread(os.path.join(path_categories, img), self.GRAY_IMG)\n",
        "      plt.imshow(img_array)\n",
        "      new_img_array = cv2.resize(img_array, (self.IMG_SIZE, self.IMG_SIZE))\n",
        "      test_loss, test_acc = model.evaluate(new_img_array,  class_index, verbose=2)\n",
        "      print(test_loss)\n",
        "      time.sleep(1000)\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lEHNMJsO15H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}